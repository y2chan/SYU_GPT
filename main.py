import os
import streamlit as st
from langchain import hub
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import DirectoryLoader
from functools import lru_cache

# í™˜ê²½ ì„¤ì •
def setup_environment():
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
    os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGSMITH_API_TOKEN")
    os.environ["LANGCHAIN_PROJECT"] = "SYU-GPT"
    os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
    os.environ["SERPER_API_KEY"] = os.getenv("SERPER_API_KEY")

# ë¬¸ì„œ ì²˜ë¦¬ ì¤€ë¹„
def prepare_documents():
    if "retrievers" not in st.session_state:
        st.session_state.retrievers = []

    # íŒŒì¼ë³„ ì„¤ì •
    config = {
        'introduce.txt': {'chunk_size': 1500, 'chunk_overlap': 300},
        'ê´€ë ¨ ë§í¬ data.txt': {'chunk_size': 1000, 'chunk_overlap': 200},
        'êµí†µ data.txt': {'chunk_size': 1500, 'chunk_overlap': 300},
        'ë„ì„œê´€ data.txt': {'chunk_size': 2000, 'chunk_overlap': 300},
        'ë™ì•„ë¦¬ data.txt': {'chunk_size': 1500, 'chunk_overlap': 250},
        'ë“±ë¡ data.txt': {'chunk_size': 1500, 'chunk_overlap': 250},
        'ì„±ì  data.txt': {'chunk_size': 1500, 'chunk_overlap': 300},
        'ì…”í‹€ë²„ìŠ¤ data.txt': {'chunk_size': 2000, 'chunk_overlap': 300},
        'ìˆ˜ê°•ì‹ ì²­ data.txt': {'chunk_size': 1500, 'chunk_overlap': 250},
        'ì‹œì„¤ ì •ë³´ data.txt': {'chunk_size': 2000, 'chunk_overlap': 350},
        'ì—…ë¬´ë³„ ì „í™”ë²ˆí˜¸ data.txt': {'chunk_size': 1000, 'chunk_overlap': 200},
        'ì¥í•™ê¸ˆ data.txt': {'chunk_size': 1200, 'chunk_overlap': 250},
        'ì¡¸ì—… data.txt': {'chunk_size': 1200, 'chunk_overlap': 250},
        'ì¦ëª…ì„œ data.txt': {'chunk_size': 1200, 'chunk_overlap': 250},
        'í•™ê³¼ data.txt': {'chunk_size': 1200, 'chunk_overlap': 250},
        'í•™ì‚¬ ì¼ì • data.txt': {'chunk_size': 1500, 'chunk_overlap': 300},
        'í›„ë¬¸ ì •ë³´ data.txt': {'chunk_size': 1000, 'chunk_overlap': 200},
        'í•™êµ ê±´ë¬¼ data.txt': {'chunk_size': 2000, 'chunk_overlap': 300},
    }

    # DirectoryLoaderë¡œ ëª¨ë“  txt íŒŒì¼ ë¡œë“œ
    loader = DirectoryLoader(".", glob="data/SYU_GPT/*.txt", show_progress=True)
    docs = loader.load()

    all_splits = []
    for doc in docs:
        file_path = doc.metadata['source']
        file_name = os.path.basename(file_path)

        if file_name in config:
            chunk_size = config[file_name]['chunk_size']
            chunk_overlap = config[file_name]['chunk_overlap']
        else:
            chunk_size = 1500
            chunk_overlap = 300

        text_splitter = CharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        splits = text_splitter.split_documents([doc])
        all_splits.extend(splits)

    # ëª¨ë“  ë¶„í• ì´ ì™„ë£Œëœ í›„ì— í•œ ë²ˆë§Œ vectorstoreë¥¼ ìƒì„±
    if all_splits:
        vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
        st.session_state.retrievers.append(vectorstore.as_retriever())
    else:
        print("No documents were split or processed.")

# ì‘ë‹µ ìƒì„±
@lru_cache(maxsize=100)  # ìµœëŒ€ 100ê°œì˜ ìœ ë‹ˆí¬ ìš”ì²­ì„ ìºì‹œ
def generate_response(user_input):
    if "retrievers" not in st.session_state or not st.session_state.retrievers:
        return "ë¬¸ì„œ ì²˜ë¦¬ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì²˜ë¦¬í•´ì£¼ì„¸ìš”."

    try:
        retriever = st.session_state.retrievers[0]
        prompt = hub.pull("rlm/rag-prompt")
        llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, max_tokens=300)

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        rag_chain = (
                {"context": retriever | format_docs, "question": RunnablePassthrough()}
                | prompt
                | llm
                | StrOutputParser()
        )

        response = rag_chain.invoke(user_input)
        return response
    except Exception as e:
        st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìì„¸í•œ ì •ë³´ëŠ” ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."

def main():
    st.set_page_config(
        page_title="SYU-GPT",
        # page_icon="ğŸ˜ƒ",
        page_icon="photo/Logo.png",
        layout="wide",
        initial_sidebar_state="auto",
        menu_items={
            'Get Help': 'https://www.extremelycoolapp.com/help',
            'Report a bug': "https://www.extremelycoolapp.com/bug",
        }
    )

    st.title('SYU-GPT', anchor=False)

    # ë¨¼ì €, subheaderì™€ captionì„ í¬í•¨í•˜ëŠ” ë¶€ë¶„ì„ st.empty()ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ˆ í™€ë”ë¡œ ë§Œë“­ë‹ˆë‹¤.
    info_placeholder = st.empty()

    # ì´ì œ info_placeholderë¥¼ ì‚¬ìš©í•˜ì—¬ subheaderì™€ captionì„ í‘œì‹œí•©ë‹ˆë‹¤.
    with info_placeholder.container():
        st.subheader('ì‚¼ìœ¡ëŒ€í•™êµ ê²€ìƒ‰ ì—”ì§„', anchor=False)
        st.caption('ì—¬ëŸ¬ë¶„ì´ ê²€ìƒ‰í•˜ê³  ì‹¶ì€ í•™êµ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì„¸ìš”!')
        st.caption('ë°ì´í„°ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ì¤‘ì…ë‹ˆë‹¤.')
        st.caption('ì‚¼ìœ¡ëŒ€í•™êµ ì¬í•™ìƒì´ë¼ë©´ ì‚¬ìš©í•´ë³´ì„¸ìš”! ğŸ˜Š')

    st.caption('ì‚¬ìš©í•˜ì‹œëŠ”ë° ë¶ˆí¸í•œ ì ì´ ìˆìœ¼ë©´ ì•„ë˜ ì‚¬ìš© ê°€ì´ë“œë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!')
    st.caption(' ')
    st.page_link("pages/guide.py", label="ì‚¬ìš© ê°€ì´ë“œ ë°”ë¡œê°€ê¸°", help="ì‚¬ìš© ê°€ì´ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤.", icon="â–¶")

    # ì‚¬ì´ë“œë°”
    st.sidebar.image("photo/syugptLogo.png")
    hide_img_fs = '''
    <style>
    button[title="View fullscreen"]{
        visibility: hidden;}
    </style>
    '''
    st.sidebar.markdown(hide_img_fs, unsafe_allow_html=True)

    st.sidebar.write('-' * 50)
    st.sidebar.subheader("Menu")
    st.sidebar.page_link("main.py", label="Home", help="í™ˆ í™”ë©´ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤", icon="ğŸ ")
    st.sidebar.page_link("pages/greeting.py", label="Greeting", icon="âœ‹")
    st.sidebar.page_link("pages/guide.py", label="User's Guide", icon="â“")
    st.sidebar.subheader("Other Web")
    st.sidebar.page_link("https://www.syu.ac.kr/", label="Sahmyook University", help="ì‚¼ìœ¡ëŒ€í•™êµ ê³µì‹ ì‚¬ì´íŠ¸ë¡œ ì´ë™í•©ë‹ˆë‹¤")
    st.sidebar.page_link("https://chat.openai.com/", label="ChatGPT", help="Chat GPT ì‚¬ì´íŠ¸ë¡œ ì´ë™í•©ë‹ˆë‹¤")
    st.sidebar.page_link("https://gabean.kr/", label="GaBean", help="ê°œë°œìì˜ ë˜ ë‹¤ë¥¸ ì›¹ ì‚¬ì´íŠ¸ë¡œ ì´ë™í•©ë‹ˆë‹¤")

    if "chat_session" not in st.session_state:
        st.session_state.messages = []

    if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
        info_placeholder.empty()
        setup_environment()
        try:
            prepare_documents()
            # ê¸°íƒ€ ì½”ë“œ êµ¬í˜„
        except Exception as e:
            st.error(f"An unexpected error occurred: {e}")

        try:
            with st.spinner("ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                response = generate_response(user_input)

            with st.chat_message("user", avatar="ğŸ§ƒ"):
                st.markdown(user_input)
            st.session_state.messages.append({"role": "user", "content": user_input})
            with st.chat_message("SYU-GPT", avatar="photo/Logo.png"):
                st.markdown(response)
            st.session_state.messages.append({"role": "SYU-GPT", "content": response})
        except Exception as e:
            st.error("ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {}".format(e))

if __name__ == "__main__":
    main()